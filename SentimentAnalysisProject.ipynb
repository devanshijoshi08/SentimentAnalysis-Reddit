{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis for Reddit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\devuj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\devuj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\devuj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\devuj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.\tData Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id='_DImiptAS4SzysaUp5ZhUQ',\n",
    "                     client_secret='D4PNJf-0_ddWTJtmL9egt-Gie48pzQ',\n",
    "                     user_agent='Responsible-Art5268')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total headlines collected: 10000\n"
     ]
    }
   ],
   "source": [
    "# Different sorts and time filters\n",
    "sort_methods = ['new', 'hot', 'top', 'controversial']\n",
    "time_filters = ['day', 'week', 'month', 'year', 'all']\n",
    "\n",
    "# List of related subreddits\n",
    "related_subreddits = ['economy', 'finance', 'business', 'investing']\n",
    "\n",
    "# Function to fetch headlines from a given subreddit\n",
    "def fetch_headlines_from_subreddit(subreddit_name):\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    for sort_method in sort_methods:\n",
    "        if sort_method in ['top', 'controversial']:\n",
    "            for time_filter in time_filters:\n",
    "                fetch_headlines(subreddit, sort_method, time_filter)\n",
    "                if len(headlines) >= 10000:\n",
    "                    return\n",
    "        else:\n",
    "            fetch_headlines(subreddit, sort_method)\n",
    "            if len(headlines) >= 10000:\n",
    "                return\n",
    "\n",
    "# Function to fetch headlines using a specific sort and time filter\n",
    "def fetch_headlines(subreddit, sort_method, time_filter=None):\n",
    "    if time_filter:\n",
    "        submissions = getattr(subreddit, sort_method)(time_filter=time_filter, limit=None)\n",
    "    else:\n",
    "        submissions = getattr(subreddit, sort_method)(limit=None)\n",
    "\n",
    "    for submission in submissions:\n",
    "        headlines.add(submission.title)\n",
    "        if len(headlines) >= 10000:\n",
    "            break\n",
    "\n",
    "# Iterate and collect headlines from each subreddit\n",
    "for subreddit_name in related_subreddits:\n",
    "    fetch_headlines_from_subreddit(subreddit_name)\n",
    "    if len(headlines) >= 10000:\n",
    "        break\n",
    "\n",
    "print(f\"Total headlines collected: {len(headlines)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting to lowercase\n",
    "lowercaseHeadlines = set()\n",
    "for item in headlines:\n",
    "    lowercaseHeadlines.add(item.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing the special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items with special characters: 8060\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Function to remove special characters\n",
    "def remove_special_characters(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "\n",
    "# Clean the headlines and create a new set\n",
    "cleaned_headlines = {remove_special_characters(item) for item in lowercaseHeadlines}\n",
    "\n",
    "# Counting the number of headlines with special characters\n",
    "count_special = sum(bool(re.search(r'[^a-zA-Z0-9\\s]', item)) for item in lowercaseHeadlines)\n",
    "\n",
    "print(f\"Number of items with special characters: {count_special}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotate the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', 'neutral'), ('warren buffett is now richer than mark zuckerberg after tech titan lost 31 billion following metas stock crash', 'negative'), ('car loans could be the next subprime crisis thanks republicans', 'negative'), ('how bonds ate the entire financial system  a very short very wild history of the market that will shape the next financial crisis', 'negative'), ('china produces more automobiles than the us japan south korea and germany combined', 'neutral')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "annotated_headlines = []\n",
    "for headline in cleaned_headlines:\n",
    "    score = sia.polarity_scores(headline)\n",
    "    compound = score['compound']\n",
    "    \n",
    "    if compound >= 0.05:\n",
    "        sentiment = 'positive'\n",
    "    elif compound <= -0.05:\n",
    "        sentiment = 'negative'\n",
    "    else:\n",
    "        sentiment = 'neutral'\n",
    "\n",
    "    annotated_headlines.append((headline, sentiment))\n",
    "\n",
    "# Example: print first few annotated headlines\n",
    "print(annotated_headlines[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>warren buffett is now richer than mark zuckerb...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>car loans could be the next subprime crisis th...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>how bonds ate the entire financial system  a v...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>china produces more automobiles than the us ja...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headline     Label\n",
       "0                                                      neutral\n",
       "1  warren buffett is now richer than mark zuckerb...  negative\n",
       "2  car loans could be the next subprime crisis th...  negative\n",
       "3  how bonds ate the entire financial system  a v...  negative\n",
       "4  china produces more automobiles than the us ja...   neutral"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headline_df = pd.DataFrame.from_records(annotated_headlines, columns=['Headline', 'Label'])\n",
    "headline_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "csv_file_path = 'annotated_headlines.csv'  \n",
    "headline_df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove any null headlines present after the preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>warren buffett is now richer than mark zuckerb...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>car loans could be the next subprime crisis th...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>how bonds ate the entire financial system  a v...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>china produces more automobiles than the us ja...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>biden defends his handling of the economy amid...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headline     Label\n",
       "1  warren buffett is now richer than mark zuckerb...  negative\n",
       "2  car loans could be the next subprime crisis th...  negative\n",
       "3  how bonds ate the entire financial system  a v...  negative\n",
       "4  china produces more automobiles than the us ja...   neutral\n",
       "5  biden defends his handling of the economy amid...   neutral"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalHeadline_df = headline_df[headline_df['Headline'].notna() & headline_df['Headline'].str.strip().astype(bool)]\n",
    "finalHeadline_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9987, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headline_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.\tPreprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Label</th>\n",
       "      <th>Processed_Headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>warren buffett is now richer than mark zuckerb...</td>\n",
       "      <td>negative</td>\n",
       "      <td>warren buffett richer mark zuckerberg tech tit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>car loans could be the next subprime crisis th...</td>\n",
       "      <td>negative</td>\n",
       "      <td>car loan could next subprim crisi thank republ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>how bonds ate the entire financial system  a v...</td>\n",
       "      <td>negative</td>\n",
       "      <td>bond ate entir financi system short wild histo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>china produces more automobiles than the us ja...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>china produc automobil u japan south korea ger...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>biden defends his handling of the economy amid...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>biden defend handl economi amid latest rough i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headline     Label  \\\n",
       "1  warren buffett is now richer than mark zuckerb...  negative   \n",
       "2  car loans could be the next subprime crisis th...  negative   \n",
       "3  how bonds ate the entire financial system  a v...  negative   \n",
       "4  china produces more automobiles than the us ja...   neutral   \n",
       "5  biden defends his handling of the economy amid...   neutral   \n",
       "\n",
       "                                  Processed_Headline  \n",
       "1  warren buffett richer mark zuckerberg tech tit...  \n",
       "2  car loan could next subprim crisi thank republ...  \n",
       "3  bond ate entir financi system short wild histo...  \n",
       "4  china produc automobil u japan south korea ger...  \n",
       "5  biden defend handl economi amid latest rough i...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenizing the words\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Removing non-alpha characters\n",
    "    tokens = [word for word in tokens if word.isalpha()] \n",
    "\n",
    "    # Removing stopwords\n",
    "    stop_words = set(stopwords.words('english'))    \n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Stemming and Lemmatizing the words\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(stemmer.stem(word)) for word in tokens] \n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply the preprocess_text function using .loc\n",
    "finalHeadline_df.loc[:, 'Processed_Headline'] = finalHeadline_df['Headline'].apply(preprocess_text)\n",
    "finalHeadline_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.\tFeature Extraction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(finalHeadline_df['Processed_Headline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Performance (Scaled):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.68      0.67      0.68       700\n",
      "     neutral       0.63      0.70      0.66       658\n",
      "    positive       0.69      0.63      0.66       640\n",
      "\n",
      "    accuracy                           0.67      1998\n",
      "   macro avg       0.67      0.67      0.67      1998\n",
      "weighted avg       0.67      0.67      0.67      1998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(tfidf_features, finalHeadline_df['Label'], test_size=0.2, random_state=42)\n",
    "\n",
    "scaler_tfidf = StandardScaler()\n",
    "X_train_tfidf_scaled = scaler_tfidf.fit_transform(X_train_tfidf.toarray()) \n",
    "X_test_tfidf_scaled = scaler_tfidf.transform(X_test_tfidf.toarray())  \n",
    "classifier_tfidf = LogisticRegression(max_iter=1000)\n",
    "classifier_tfidf.fit(X_train_tfidf_scaled, y_train_tfidf)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_tfidf = classifier_tfidf.predict(X_test_tfidf_scaled)\n",
    "\n",
    "# Evaluate the classifier\n",
    "print(\"TF-IDF Performance (Scaled):\")\n",
    "print(classification_report(y_test_tfidf, y_pred_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# Tokenized documents\n",
    "tokenized_docs = [doc.split() for doc in finalHeadline_df['Processed_Headline']]\n",
    "\n",
    "# Train a Word2Vec model\n",
    "word2vec_model = gensim.models.Word2Vec(tokenized_docs, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Function to create document vectors\n",
    "def document_vector_word2vec(doc):\n",
    "    words = doc.split()\n",
    "    word_vectors = [word2vec_model.wv[word] for word in words if word in word2vec_model.wv]\n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(word2vec_model.vector_size)  # Return a zero vector if no words are found\n",
    "    else:\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "\n",
    "word2vec_features = np.array([document_vector_word2vec(doc) for doc in finalHeadline_df['Processed_Headline']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.41      0.66      0.50       700\n",
      "     neutral       0.41      0.50      0.45       658\n",
      "    positive       0.49      0.05      0.09       640\n",
      "\n",
      "    accuracy                           0.41      1998\n",
      "   macro avg       0.43      0.40      0.35      1998\n",
      "weighted avg       0.43      0.41      0.35      1998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec\n",
    "X_train_word2vec, X_test_word2vec, y_train_word2vec, y_test_word2vec = train_test_split(word2vec_features, finalHeadline_df['Label'], test_size=0.2, random_state=42)\n",
    "classifier_word2vec = LogisticRegression()\n",
    "classifier_word2vec.fit(X_train_word2vec, y_train_word2vec)\n",
    "y_pred_word2vec = classifier_word2vec.predict(X_test_word2vec)\n",
    "\n",
    "# Evaluate the classifier\n",
    "print(\"Word2Vec Performance:\")\n",
    "print(classification_report(y_test_word2vec, y_pred_word2vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Load pre-trained GloVe model (choose an appropriate model)\n",
    "glove_model = api.load(\"glove-wiki-gigaword-100\")  # For example\n",
    "\n",
    "# Function to create document vectors\n",
    "def document_vector_glove(doc):\n",
    "    words = doc.split()\n",
    "    word_vectors = [glove_model[word] for word in words if word in glove_model]\n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(glove_model.vector_size)  # Return a zero vector if no words are found\n",
    "    else:\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "\n",
    "glove_features = np.array([document_vector_glove(doc) for doc in finalHeadline_df['Processed_Headline']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.55      0.76      0.64       700\n",
      "     neutral       0.64      0.29      0.40       658\n",
      "    positive       0.54      0.62      0.57       640\n",
      "\n",
      "    accuracy                           0.56      1998\n",
      "   macro avg       0.58      0.56      0.54      1998\n",
      "weighted avg       0.58      0.56      0.54      1998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GloVe\n",
    "X_train_glove, X_test_glove, y_train_glove, y_test_glove = train_test_split(glove_features, finalHeadline_df['Label'], test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_glove_scaled = scaler.fit_transform(X_train_glove)\n",
    "X_test_glove_scaled = scaler.transform(X_test_glove)\n",
    "\n",
    "classifier_glove = LogisticRegression(max_iter=1000)\n",
    "classifier_glove.fit(X_train_glove_scaled, y_train_glove)\n",
    "y_pred_glove = classifier_glove.predict(X_test_glove)\n",
    "\n",
    "# Evaluate the classifier\n",
    "print(\"GloVe Performance:\")\n",
    "print(classification_report(y_test_glove, y_pred_glove))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating which Feature Extraction performs better among the three \n",
    "1. **TF-IDF Performance:**\n",
    "    - Accuracy: 67% \n",
    "    - Precision, Recall, F1-Score: The three sentiment classifications are very equally distributed.\n",
    "    - Observation: TF-IDF appears to function fairly well, indicating that the frequency and distinctiveness of the context in the headlines serve as reliable markers of mood. Recall and precision seem to be fairly balanced with this strategy.\n",
    "    <br><br>\n",
    "\n",
    "2. **Word2Vec Performance:**\n",
    "    - Accuracy: 43%\n",
    "    - Precision, Recall, and F1-Score: Much worse results, particularly with the positive class that has essentially no recall.\n",
    "    - Observation: The low performance could be attributed to the Word2Vec model's inability to adequately capture semantic relationships in the data, or to the peculiar vocabulary prevalent in Reddit headlines that isn't adequately represented in the Word2Vec model's training corpus.\n",
    "    <br><br>\n",
    "3. **GloVe Performance:**\n",
    "    - Accuracy: 55%\n",
    "    - Precision, Recall, and F1-Score: Not as good as TF-IDF, but still superior to Word2Vec. In contrast, GloVe has a poorer recall but a comparatively higher precision.\n",
    "    - Observation: GloVe's performance indicates that although it is superior to Word2Vec in capturing semantic associations, it is not as good as TF-IDF in providing context-specific understanding for this specific dataset.\n",
    "\n",
    "### Insights about the dataset based on the feature extraction techniques:\n",
    "- **Best Overall Method:** In terms of overall accuracy and balance between precision, recall, and F1-score, TF-IDF performs better than GloVe and Word2Vec. This suggests that, for the data collected, Word2Vec or GloVe's semantic word associations are not as predictive of sentiment as TF-IDF's capture of the value of individual terms.\n",
    "- **Dataset Specificity:** Word2Vec and GloVe's performance indicates that the collected data may contain unique terminology or certain contextual nuances that these algorithms are not completely capturing. This may occur in datasets that contain slang, specialised terminology, or inventive language use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Label</th>\n",
       "      <th>Processed_Headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>warren buffett is now richer than mark zuckerb...</td>\n",
       "      <td>negative</td>\n",
       "      <td>warren buffett richer mark zuckerberg tech tit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>car loans could be the next subprime crisis th...</td>\n",
       "      <td>negative</td>\n",
       "      <td>car loan could next subprim crisi thank republ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>how bonds ate the entire financial system  a v...</td>\n",
       "      <td>negative</td>\n",
       "      <td>bond ate entir financi system short wild histo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>china produces more automobiles than the us ja...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>china produc automobil u japan south korea ger...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>biden defends his handling of the economy amid...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>biden defend handl economi amid latest rough i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9982</th>\n",
       "      <td>deutsche bank ceo warns recession is inevitabl...</td>\n",
       "      <td>negative</td>\n",
       "      <td>deutsch bank ceo warn recess inevit say german...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9983</th>\n",
       "      <td>president biden to join uaw picket line and fi...</td>\n",
       "      <td>positive</td>\n",
       "      <td>presid biden join uaw picket line fix u economi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9984</th>\n",
       "      <td>americans are fed up with billionaires washing...</td>\n",
       "      <td>positive</td>\n",
       "      <td>american fed billionair washington need get we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>where to start</td>\n",
       "      <td>neutral</td>\n",
       "      <td>start</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>americans support for labor unions at highest ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>american support labor union highest nearli year</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9986 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Headline     Label  \\\n",
       "1     warren buffett is now richer than mark zuckerb...  negative   \n",
       "2     car loans could be the next subprime crisis th...  negative   \n",
       "3     how bonds ate the entire financial system  a v...  negative   \n",
       "4     china produces more automobiles than the us ja...   neutral   \n",
       "5     biden defends his handling of the economy amid...   neutral   \n",
       "...                                                 ...       ...   \n",
       "9982  deutsche bank ceo warns recession is inevitabl...  negative   \n",
       "9983  president biden to join uaw picket line and fi...  positive   \n",
       "9984  americans are fed up with billionaires washing...  positive   \n",
       "9985                                     where to start   neutral   \n",
       "9986  americans support for labor unions at highest ...  positive   \n",
       "\n",
       "                                     Processed_Headline  \n",
       "1     warren buffett richer mark zuckerberg tech tit...  \n",
       "2     car loan could next subprim crisi thank republ...  \n",
       "3     bond ate entir financi system short wild histo...  \n",
       "4     china produc automobil u japan south korea ger...  \n",
       "5     biden defend handl economi amid latest rough i...  \n",
       "...                                                 ...  \n",
       "9982  deutsch bank ceo warn recess inevit say german...  \n",
       "9983    presid biden join uaw picket line fix u economi  \n",
       "9984  american fed billionair washington need get we...  \n",
       "9985                                              start  \n",
       "9986   american support labor union highest nearli year  \n",
       "\n",
       "[9986 rows x 3 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalHeadline_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.\tModel Selection and Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classifier Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.84      0.75       700\n",
      "     neutral       0.75      0.65      0.69       658\n",
      "    positive       0.75      0.65      0.70       640\n",
      "\n",
      "    accuracy                           0.71      1998\n",
      "   macro avg       0.72      0.71      0.71      1998\n",
      "weighted avg       0.72      0.71      0.71      1998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_features, finalHeadline_df['Label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Naive Bayes Classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "print(\"Naive Bayes Classifier Performance:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Classifier Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.74      0.77       700\n",
      "     neutral       0.71      0.85      0.77       658\n",
      "    positive       0.81      0.71      0.75       640\n",
      "\n",
      "    accuracy                           0.77      1998\n",
      "   macro avg       0.77      0.77      0.76      1998\n",
      "weighted avg       0.77      0.77      0.76      1998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initialize and train the SVM Classifier\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier\n",
    "y_pred_svm = svm_classifier.predict(X_test)\n",
    "print(\"SVM Classifier Performance:\")\n",
    "print(classification_report(y_test, y_pred_svm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.8881 - accuracy: 0.5658\n",
      "Epoch 2/10\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3581 - accuracy: 0.8739\n",
      "Epoch 3/10\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.1454 - accuracy: 0.9591\n",
      "Epoch 4/10\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.0597 - accuracy: 0.9840\n",
      "Epoch 5/10\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.0221 - accuracy: 0.9947\n",
      "Epoch 6/10\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.0068 - accuracy: 0.9990\n",
      "Epoch 7/10\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.0025 - accuracy: 0.9997\n",
      "Epoch 8/10\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 7.0575e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 4.9041e-04 - accuracy: 1.0000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 8.4237 - accuracy: 0.1316\n",
      "Neural Network Performance: Accuracy = 0.13\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Convert labels to categorical\n",
    "y_train_cat = to_categorical(y_train.factorize()[0])\n",
    "y_test_cat = to_categorical(y_test.factorize()[0])\n",
    "\n",
    "# Neural Network Model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(y_train_cat.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train.toarray(), y_train_cat, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test.toarray(), y_test_cat)\n",
    "print(\"Neural Network Performance: Accuracy = {:.2f}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      0.11      0.11       658\n",
      "           1       0.13      0.13      0.13       640\n",
      "           2       0.15      0.15      0.15       700\n",
      "\n",
      "    accuracy                           0.13      1998\n",
      "   macro avg       0.13      0.13      0.13      1998\n",
      "weighted avg       0.13      0.13      0.13      1998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(y_test_cat, axis=1)\n",
    "\n",
    "report = classification_report(y_true, y_pred_classes)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.\tDeployment and Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I love this new phone, it has such great features!\n",
      "Predicted Sentiment: positive\n",
      "\n",
      "Text: This is the worst movie I have ever seen.\n",
      "Predicted Sentiment: negative\n",
      "\n",
      "Text: I'm not really sure how I feel about this new policy.\n",
      "Predicted Sentiment: positive\n",
      "\n",
      "Text: What a terrible way to handle the situation!\n",
      "Predicted Sentiment: negative\n",
      "\n",
      "Text: Absolutely adore the new restaurant in town!\n",
      "Predicted Sentiment: neutral\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "svm_classifier.classes_ = np.array(['negative', 'neutral', 'positive'])\n",
    "\n",
    "def predict_sentiment(text):\n",
    "    processed_input = preprocess_text(text) \n",
    "    vectorized_input = tfidf_vectorizer.transform([processed_input])\n",
    "    prediction = svm_classifier.predict(vectorized_input)\n",
    "    return prediction[0]\n",
    "\n",
    "test_texts = [\n",
    "    \"I love this new phone, it has such great features!\",\n",
    "    \"This is the worst movie I have ever seen.\",\n",
    "    \"I'm not really sure how I feel about this new policy.\",\n",
    "    \"What a terrible way to handle the situation!\",\n",
    "    \"Absolutely adore the new restaurant in town!\"\n",
    "]\n",
    "\n",
    "# Using the function to predict sentiment for each test text\n",
    "predicted_sentiments = {text: predict_sentiment(text) for text in test_texts}\n",
    "\n",
    "# Printing the predicted sentiments for each test text\n",
    "for text, sentiment in predicted_sentiments.items():\n",
    "    print(f\"Text: {text}\\nPredicted Sentiment: {sentiment}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Function to preprocess and predict sentiment\n",
    "def predict_sentiment():\n",
    "    user_input = text_input.get(\"1.0\", \"end-1c\")  \n",
    "    processed_input = preprocess_text(user_input)  \n",
    "    vectorized_input = tfidf_vectorizer.transform([processed_input])  \n",
    "    prediction = svm_classifier.predict(vectorized_input)  \n",
    "    result_label.config(text=\"Predicted Sentiment: \" + str(prediction[0]))  \n",
    "\n",
    "# Tkinter window\n",
    "root = tk.Tk()\n",
    "root.title(\"Social Media Sentiment Analysis\")\n",
    "\n",
    "# Text input widget\n",
    "text_input = tk.Text(root, height=5, width=40)\n",
    "text_input.pack()\n",
    "\n",
    "# Predict button\n",
    "predict_button = tk.Button(root, text=\"Predict Sentiment\", command=predict_sentiment)\n",
    "predict_button.pack()\n",
    "\n",
    "# Label to display the result\n",
    "result_label = tk.Label(root, text=\"Predicted Sentiment: \")\n",
    "result_label.pack()\n",
    "\n",
    "# Run the application\n",
    "root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSCN8010_classic_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
